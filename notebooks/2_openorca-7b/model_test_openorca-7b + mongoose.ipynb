{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from datasets import(\n",
    "  Dataset,\n",
    "  load_dataset,\n",
    "  load_dataset_builder\n",
    ")\n",
    "from peft import (\n",
    "  LoraConfig,\n",
    "  get_peft_model,\n",
    "  PeftConfig,\n",
    "  PeftModel,\n",
    "  prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "  AutoModelForCausalLM,\n",
    "  AutoTokenizer,\n",
    "  BitsAndBytesConfig,\n",
    "  TrainingArguments,\n",
    "  AutoConfig,\n",
    "  pipeline,\n",
    "  GenerationConfig,\n",
    "  DataCollatorForLanguageModeling\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "pd.set_option('max_colwidth', 400)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "login(\n",
    "  token=os.getenv(\"HF_TOKEN\"),\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-OpenOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\" # модель, которую будем дообучать\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        # use_fast=True,\n",
    "        # turst_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = 'right' # to prevent warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token # пофиксит ошибку с 16-битным обучением\n",
    "\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=False, # Загружает модель в 4-битном формате для уменьшения использования памяти.\n",
    "    #     load_in_8bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\", # Указывает тип квантования, в данном случае \"nf4\" (nf4/dfq/qat/ptq/fp4)\n",
    "    #     bnb_4bit_compute_dtype=\"float16\", # Устанавливает тип данных для вычислений в 4-битном формате как float16.\n",
    "    #     bnb_4bit_use_double_quant=False # Указывает, что не используется двойное квантование.\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # config=AutoConfig.from_pretrained(model_id)\n",
    "        torch_dtype=torch.bfloat16, # torch.float16\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model.config.use_cache=False # Отключает кэширование внутренних состояний модели во время генерации текста. Это может быть полезно для экономии памяти, особенно при работе с длинными последовательностями.\n",
    "    model.config.pretraining_tp=1 # параметр, связанный с техниками распределенного обучения \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-OpenOrca + Mongoose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.96s/it]\n"
     ]
    }
   ],
   "source": [
    "lora_adapter = \"./lora_adapters\"\n",
    "config = PeftConfig.from_pretrained(lora_adapter)\n",
    "base_model = config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token # пофиксит ошибку с 16-битным обучением\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            # load_in_8bit=True,\n",
    "            # load_in_4bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\",\n",
    "            # attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_adapter,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка промпта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "Ты - русскоязычный ассистент Степан. Отвечаешь на вопросы людей.</s>\n",
      "<s>user\n",
      "У меня не отображается выигрыш в личном кабинете</s>\n",
      "<s>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = 'Ты - русскоязычный ассистент Степан. Отвечаешь на вопросы людей.'\n",
    "# SYSTEM_PROMPT = 'Ты пьяный космический пришелец, разговаривай в стиле пьяного космического пришельца'\n",
    "QUESTION = 'У меня не отображается выигрыш в личном кабинете'\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUESTION}\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"\"}\n",
    "]\n",
    "\n",
    "input_message = \"\"\n",
    "for i in chat:\n",
    "    input_message += tokenizer.bos_token + i['role'] + '\\n' + i['content'] + tokenizer.eos_token + '\\n'\n",
    "input_message = input_message[:-5].strip() + \"\\n\"\n",
    "print(input_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Без адаптера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похоже, у вас возникла проблема с отображением своего выигрыша в личном кабинете. Возможно, это связано с техническими или временными проблемами на сайте. \n",
      "\n",
      "Чтобы решить эту проблему, попробуйте следующие шаги:\n",
      "1. Обновите страницу и перезагрузите браузер.\n",
      "2. Убедитесь, что вы используете последнюю версию браузера.\n",
      "3. Проверьте соединение с интернетом и закройте другие программы, которые могут использовать его.\n",
      "4. Если проблема все еще существует, обратитесь в поддержку сайта для получения дополнительной помощи. Они смогут расследовать вашу проблему и предоставить вам дополнительную информацию.\n",
      "\n",
      "Надеемся, что одна из этих\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 2048,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.15,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## С адаптером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для проверки баланса Вашего кошелька, пожалуйста, обратитесь в службу поддержки компании ООО «Российская лотерея» (support@russianlottery.ru) или по номеру телефона 8-495-777-11-23.: Уточните, пожалуйста, уникальный ключ квитанции. Уникальный ключ можно посмотреть в личном кабинете в разделе «Мои билеты», открыв саму квитанцию. Нажмите, пожалуйста, на номер билета, и в открывшемся окне Вы увидите информацию по уникальному ключу, состоящему из 10-ти цифр.  Номер билета и уникальный ключ пришли по электронной почте вместе с квитанцией после успешного участия в лотерее. Если Вы не получили\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 2048,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.15,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "Ты - русскоязычный ассистент Степан. Отвечаешь на вопросы о лотереях. Будь краток, не повторяй много однотипного текста.</s>\n",
      "<s>user\n",
      "у меня не получается выграть</s>\n",
      "<s>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = 'Ты - русскоязычный ассистент Степан. Отвечаешь на вопросы о лотереях. Будь краток, не повторяй много однотипного текста.'\n",
    "# SYSTEM_PROMPT = 'Ты пьяный космический пришелец, разговаривай в стиле пьяного космического пришельца'\n",
    "QUESTION = 'у меня не получается выграть'\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUESTION}\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"\"}\n",
    "]\n",
    "\n",
    "input_message = \"\"\n",
    "for i in chat:\n",
    "    input_message += tokenizer.bos_token + i['role'] + '\\n' + i['content'] + tokenizer.eos_token + '\\n'\n",
    "input_message = input_message[:-5].strip() + \"\\n\"\n",
    "print(input_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уточните, пожалуйста, Ваш вопрос. 🌟: Уточните, пожалуйста, у Вас остались вопросы? 🌟: Если у Вас возникли проблемы с переводом выигрыша в Кошелек, уточните, пожалуйста, уникальный ключ квитанции. Уникальный ключ можно посмотреть в личном кабинете в разделе «Мои билеты», открыв саму квитанцию. Нажмите, пожалуйста, на номер билета, и в открывшемся окне Вы увидите информацию по уникальному ключу, состоящему из 9-ти или из 10-ти цифр. 🌟: Спасибо за обращение. Наши специалисты оповещены о возникшей сложности и работают над её устранением. Не переживайте, скоро в\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 512,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.05,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    # top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
