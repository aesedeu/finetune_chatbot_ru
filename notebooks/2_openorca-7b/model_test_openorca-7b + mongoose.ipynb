{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from datasets import(\n",
    "  Dataset,\n",
    "  load_dataset,\n",
    "  load_dataset_builder\n",
    ")\n",
    "from peft import (\n",
    "  LoraConfig,\n",
    "  get_peft_model,\n",
    "  PeftConfig,\n",
    "  PeftModel,\n",
    "  prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "  AutoModelForCausalLM,\n",
    "  AutoTokenizer,\n",
    "  BitsAndBytesConfig,\n",
    "  TrainingArguments,\n",
    "  AutoConfig,\n",
    "  pipeline,\n",
    "  GenerationConfig,\n",
    "  DataCollatorForLanguageModeling\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "pd.set_option('max_colwidth', 400)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "login(\n",
    "  token=os.getenv(\"HF_TOKEN\"),\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-OpenOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\" # –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –¥–æ–æ–±—É—á–∞—Ç—å\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        # use_fast=True,\n",
    "        # turst_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = 'right' # to prevent warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token # –ø–æ—Ñ–∏–∫—Å–∏—Ç –æ—à–∏–±–∫—É —Å 16-–±–∏—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=False, # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ 4-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.\n",
    "    #     load_in_8bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\", # –£–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ \"nf4\" (nf4/dfq/qat/ptq/fp4)\n",
    "    #     bnb_4bit_compute_dtype=\"float16\", # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ 4-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∫–∞–∫ float16.\n",
    "    #     bnb_4bit_use_double_quant=False # –£–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ.\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # config=AutoConfig.from_pretrained(model_id)\n",
    "        torch_dtype=torch.bfloat16, # torch.float16\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model.config.use_cache=False # –û—Ç–∫–ª—é—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏.\n",
    "    model.config.pretraining_tp=1 # –ø–∞—Ä–∞–º–µ—Ç—Ä, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-OpenOrca + Mongoose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/finetune_chatbot_ru/.venv/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:21<00:00, 10.96s/it]\n"
     ]
    }
   ],
   "source": [
    "lora_adapter = \"./lora_adapters\"\n",
    "config = PeftConfig.from_pretrained(lora_adapter)\n",
    "base_model = config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token # –ø–æ—Ñ–∏–∫—Å–∏—Ç –æ—à–∏–±–∫—É —Å 16-–±–∏—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            # load_in_8bit=True,\n",
    "            # load_in_4bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\",\n",
    "            # attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_adapter,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–º–ø—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "–¢—ã - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –°—Ç–µ–ø–∞–Ω. –û—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ª—é–¥–µ–π.</s>\n",
      "<s>user\n",
      "–£ –º–µ–Ω—è –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤—ã–∏–≥—Ä—ã—à –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ</s>\n",
      "<s>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = '–¢—ã - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –°—Ç–µ–ø–∞–Ω. –û—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ª—é–¥–µ–π.'\n",
    "# SYSTEM_PROMPT = '–¢—ã –ø—å—è–Ω—ã–π –∫–æ—Å–º–∏—á–µ—Å–∫–∏–π –ø—Ä–∏—à–µ–ª–µ—Ü, —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–π –≤ —Å—Ç–∏–ª–µ –ø—å—è–Ω–æ–≥–æ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏—à–µ–ª—å—Ü–∞'\n",
    "QUESTION = '–£ –º–µ–Ω—è –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤—ã–∏–≥—Ä—ã—à –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ'\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUESTION}\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"\"}\n",
    "]\n",
    "\n",
    "input_message = \"\"\n",
    "for i in chat:\n",
    "    input_message += tokenizer.bos_token + i['role'] + '\\n' + i['content'] + tokenizer.eos_token + '\\n'\n",
    "input_message = input_message[:-5].strip() + \"\\n\"\n",
    "print(input_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ë–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—Ö–æ–∂–µ, —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å–≤–æ–µ–≥–æ –≤—ã–∏–≥—Ä—ã—à–∞ –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏ –Ω–∞ —Å–∞–π—Ç–µ. \n",
      "\n",
      "–ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
      "1. –û–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –±—Ä–∞—É–∑–µ—Ä.\n",
      "2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é –±—Ä–∞—É–∑–µ—Ä–∞.\n",
      "3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º –∏ –∑–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ.\n",
      "4. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –≤—Å–µ –µ—â–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏. –û–Ω–∏ —Å–º–æ–≥—É—Ç —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–∞—à—É –ø—Ä–æ–±–ª–µ–º—É –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –≤–∞–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.\n",
      "\n",
      "–ù–∞–¥–µ–µ–º—Å—è, —á—Ç–æ –æ–¥–Ω–∞ –∏–∑ —ç—Ç–∏—Ö\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 2048,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.15,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –° –∞–¥–∞–ø—Ç–µ—Ä–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–ª–∞–Ω—Å–∞ –í–∞—à–µ–≥–æ –∫–æ—à–µ–ª—å–∫–∞, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –û–û–û ¬´–†–æ—Å—Å–∏–π—Å–∫–∞—è –ª–æ—Ç–µ—Ä–µ—è¬ª (support@russianlottery.ru) –∏–ª–∏ –ø–æ –Ω–æ–º–µ—Ä—É —Ç–µ–ª–µ—Ñ–æ–Ω–∞ 8-495-777-11-23.: –£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –∫–≤–∏—Ç–∞–Ω—Ü–∏–∏. –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ú–æ–∏ –±–∏–ª–µ—Ç—ã¬ª, –æ—Ç–∫—Ä—ã–≤ —Å–∞–º—É –∫–≤–∏—Ç–∞–Ω—Ü–∏—é. –ù–∞–∂–º–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞ –Ω–æ–º–µ—Ä –±–∏–ª–µ—Ç–∞, –∏ –≤ –æ—Ç–∫—Ä—ã–≤—à–µ–º—Å—è –æ–∫–Ω–µ –í—ã —É–≤–∏–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∫–ª—é—á—É, —Å–æ—Å—Ç–æ—è—â–µ–º—É –∏–∑ 10-—Ç–∏ —Ü–∏—Ñ—Ä.  –ù–æ–º–µ—Ä –±–∏–ª–µ—Ç–∞ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –ø—Ä–∏—à–ª–∏ –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç–µ –≤–º–µ—Å—Ç–µ —Å –∫–≤–∏—Ç–∞–Ω—Ü–∏–µ–π –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —É—á–∞—Å—Ç–∏—è –≤ –ª–æ—Ç–µ—Ä–µ–µ. –ï—Å–ª–∏ –í—ã –Ω–µ –ø–æ–ª—É—á–∏–ª–∏\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 2048,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.15,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>system\n",
      "–¢—ã - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –°—Ç–µ–ø–∞–Ω. –û—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –ª–æ—Ç–µ—Ä–µ—è—Ö. –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π –º–Ω–æ–≥–æ –æ–¥–Ω–æ—Ç–∏–ø–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.</s>\n",
      "<s>user\n",
      "—É –º–µ–Ω—è –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –≤—ã–≥—Ä–∞—Ç—å</s>\n",
      "<s>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = '–¢—ã - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –°—Ç–µ–ø–∞–Ω. –û—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –ª–æ—Ç–µ—Ä–µ—è—Ö. –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π –º–Ω–æ–≥–æ –æ–¥–Ω–æ—Ç–∏–ø–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.'\n",
    "# SYSTEM_PROMPT = '–¢—ã –ø—å—è–Ω—ã–π –∫–æ—Å–º–∏—á–µ—Å–∫–∏–π –ø—Ä–∏—à–µ–ª–µ—Ü, —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–π –≤ —Å—Ç–∏–ª–µ –ø—å—è–Ω–æ–≥–æ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏—à–µ–ª—å—Ü–∞'\n",
    "QUESTION = '—É –º–µ–Ω—è –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –≤—ã–≥—Ä–∞—Ç—å'\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUESTION}\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"\"}\n",
    "]\n",
    "\n",
    "input_message = \"\"\n",
    "for i in chat:\n",
    "    input_message += tokenizer.bos_token + i['role'] + '\\n' + i['content'] + tokenizer.eos_token + '\\n'\n",
    "input_message = input_message[:-5].strip() + \"\\n\"\n",
    "print(input_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –í–∞—à –≤–æ–ø—Ä–æ—Å. üåü: –£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É –í–∞—Å –æ—Å—Ç–∞–ª–∏—Å—å –≤–æ–ø—Ä–æ—Å—ã? üåü: –ï—Å–ª–∏ —É –í–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –≤—ã–∏–≥—Ä—ã—à–∞ –≤ –ö–æ—à–µ–ª–µ–∫, —É—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –∫–≤–∏—Ç–∞–Ω—Ü–∏–∏. –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ú–æ–∏ –±–∏–ª–µ—Ç—ã¬ª, –æ—Ç–∫—Ä—ã–≤ —Å–∞–º—É –∫–≤–∏—Ç–∞–Ω—Ü–∏—é. –ù–∞–∂–º–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞ –Ω–æ–º–µ—Ä –±–∏–ª–µ—Ç–∞, –∏ –≤ –æ—Ç–∫—Ä—ã–≤—à–µ–º—Å—è –æ–∫–Ω–µ –í—ã —É–≤–∏–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∫–ª—é—á—É, —Å–æ—Å—Ç–æ—è—â–µ–º—É –∏–∑ 9-—Ç–∏ –∏–ª–∏ –∏–∑ 10-—Ç–∏ —Ü–∏—Ñ—Ä. üåü: –°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ. –ù–∞—à–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –æ–ø–æ–≤–µ—â–µ–Ω—ã –æ –≤–æ–∑–Ω–∏–∫—à–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–¥ –µ—ë —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º. –ù–µ –ø–µ—Ä–µ–∂–∏–≤–∞–π—Ç–µ, —Å–∫–æ—Ä–æ –≤\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 512,\n",
    "    max_new_tokens = 256,\n",
    "    repetition_penalty=1.05,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    # top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(output_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
