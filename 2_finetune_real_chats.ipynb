{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.philschmid.de/fine-tune-llms-in-2024-with-trl\n",
    "\n",
    "``` bash\n",
    "pip install sentencepiece transformers trl datasets pandas numpy protobuf accelerate bitsandbytes flash-attn\n",
    "pip install torch==2.0.1\n",
    "```\n",
    "На Selectel pytroch должен быть: '2.0.1+cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия pytorch: 2.0.1+cu117\n",
      "CUDA is available\n",
      "_CudaDeviceProperties(name='NVIDIA A100-PCIE-40GB', major=8, minor=0, total_memory=40384MB, multi_processor_count=108)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Версия pytorch: {torch.__version__}\")\n",
    "\n",
    "# pip install torch==2.0.1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available')\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    print('Apple MPS is available')\n",
    "else:\n",
    "    print('No GPU available')\n",
    "# _CudaDeviceProperties(name='NVIDIA A100-PCIE-40GB', major=8, minor=0, total_memory=40384MB, multi_processor_count=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/aweeu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from datasets import(\n",
    "  Dataset,\n",
    "  load_dataset,\n",
    "  load_dataset_builder\n",
    ")\n",
    "from peft import (\n",
    "  LoraConfig,\n",
    "  get_peft_model,\n",
    "  PeftConfig,\n",
    "  PeftModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "  AutoModelForCausalLM,\n",
    "  AutoTokenizer,\n",
    "  BitsAndBytesConfig,\n",
    "  TrainingArguments,\n",
    "  AutoConfig,\n",
    "  pipeline,\n",
    "  GenerationConfig,\n",
    "  DataCollatorForLanguageModeling\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "pd.set_option('max_colwidth', 400)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "login(\n",
    "  token=os.getenv(\"HF_TOKEN\"),\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования промпта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizer class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're AI assistant\n",
      "<</SYS>>\n",
      "\n",
      "Hello, how are you? [/INST] I'm doing great. How can I help you today? </s><s>[INST] I'd like to show off how chat templating works! [/INST]\n"
     ]
    }
   ],
   "source": [
    "model_id=\"IlyaGusev/saiga2_13b_lora\" # pip install sentencepiece\n",
    "# model_id=\"Open-Orca/Mistral-7B-OpenOrca\" # pip install sentencepiece\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You're AI assistant\"},\n",
    "   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание датасета описано в другом файле, где происходила выгрузка чатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    path=\"./dataset_2/dataset_short\", # указываем путь до папки с файлами\n",
    "    # data_files=\"test_dataset.json\", # или можем указать прямой путь до файла\n",
    "    split=\"train\" # указываем что загружаем (test или train)\n",
    ")\n",
    "eval_dataset = load_dataset(\n",
    "    path=\"./dataset_2/dataset_short\", # указываем путь до папки с файлами\n",
    "    # data_files=\"test_dataset.json\", # или можем указать прямой путь до файла\n",
    "    split=\"test\" # указываем что загружаем (test или train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.apply_chat_template(train_dataset['messages'][1], tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-OpenOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.69s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\" # модель, которую будем дообучать\n",
    "lora_checkpoints=\"./lora-checkpoints\" # директория с LoRA-адаптерами, которые получим на выходе\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        # use_fast=True,\n",
    "        # turst_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = 'right' # to prevent warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token # пофиксит ошибку с 16-битным обучением\n",
    "\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=False, # Загружает модель в 4-битном формате для уменьшения использования памяти.\n",
    "    #     load_in_8bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\", # Указывает тип квантования, в данном случае \"nf4\" (nf4/dfq/qat/ptq/fp4)\n",
    "    #     bnb_4bit_compute_dtype=\"float16\", # Устанавливает тип данных для вычислений в 4-битном формате как float16.\n",
    "    #     bnb_4bit_use_double_quant=False # Указывает, что не используется двойное квантование.\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # config=AutoConfig.from_pretrained(model_id)\n",
    "        torch_dtype=torch.bfloat16, # torch.float16\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model.config.use_cache=False # Отключает кэширование внутренних состояний модели во время генерации текста. Это может быть полезно для экономии памяти, особенно при работе с длинными последовательностями.\n",
    "    model.config.pretraining_tp=1 # параметр, связанный с техниками распределенного обучения \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2-13b-hf + Saiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "lora_checkpoints=\"./lora-checkpoints\"\n",
    "\n",
    "lora_adapter = \"IlyaGusev/saiga2_13b_lora\"\n",
    "config = PeftConfig.from_pretrained(lora_adapter)\n",
    "base_model = config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token # пофиксит ошибку с 16-битным обучением\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            # load_in_8bit=True,\n",
    "            # load_in_4bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\",\n",
    "            # attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_adapter,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-hf\" # модель, которую будем дообучать\n",
    "lora_checkpoints=\"./lora-checkpoints\" # директория с LoRA-адаптерами, которые получим на выходе\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        # use_fast=True,\n",
    "        # turst_remote_code=True\n",
    "    )\n",
    "    tokenizer.padding_side = 'right' # to prevent warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token # пофиксит ошибку с 16-битным обучением\n",
    "\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=False, # Загружает модель в 4-битном формате для уменьшения использования памяти.\n",
    "    #     load_in_8bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\", # Указывает тип квантования, в данном случае \"nf4\" (nf4/dfq/qat/ptq/fp4)\n",
    "    #     bnb_4bit_compute_dtype=\"float16\", # Устанавливает тип данных для вычислений в 4-битном формате как float16.\n",
    "    #     bnb_4bit_use_double_quant=False # Указывает, что не используется двойное квантование.\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # config=AutoConfig.from_pretrained(model_id)\n",
    "        torch_dtype=torch.bfloat16, # torch.float16\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model.config.use_cache=False\n",
    "    model.config.pretraining_tp=1\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there're no parameters onto CPU\n",
    "for n, p in model.named_parameters():\n",
    "    if p.device.type == 'meta':\n",
    "        print(f\"{n} is on meta!\")\n",
    "\n",
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка количества параметров в модели для обучения\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    all_params = 0\n",
    "\n",
    "    print(\"Trainable parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\"    {name}\")\n",
    "        else:\n",
    "            non_trainable_params += param.numel()\n",
    "\n",
    "    print(\"\\nNon-Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad():\n",
    "            print(f\"    {name}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSummary:\\n  Trainable params: {trainable_params}\\n  Non-Trainable params: {non_trainable_params}\\n  All params: {all_params}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:234: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/115 [00:00<?, ? examples/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "Map: 100%|██████████| 115/115 [00:00<00:00, 1665.23 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 1899.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# НАСТРОИТЬ!!!\n",
    "\n",
    "# from peft import prepare_model_for_kbit_training\n",
    "# model = prepare_model_for_kbit_training(model) # только для QLoRA\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=8, # определяет ранг матриц адаптации в LoRA. Этот параметр контролирует количество параметров адаптации, которые добавляются к модели. Более низкий ранг означает меньше дополнительных параметров и, следовательно, меньшее влияние на исходные веса модели.\n",
    "        lora_alpha=32, # масштабирующий коэффициент для матриц адаптации. Этот коэффициент управляет степенью, с которой адаптированные веса влияют на поведение модели.\n",
    "        lora_dropout=0.05,\n",
    "        # target_modules=[\n",
    "        #     \"q_proj\",\n",
    "        #     \"o_proj\",\n",
    "        #     \"k_proj\",\n",
    "        #     \"v_proj\"\n",
    "        # ]\n",
    "        bias=\"none\", # указывает, должен ли добавляться смещение (bias) к параметрам LoRA\n",
    "        task_type=\"CAUSAL_LM\", # \"CAUSAL_LM\" означает причинно-следственное (каузальное) языковое моделирование ????????????\n",
    "        # target_modules=\"all-linear\"\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=lora_checkpoints, # Путь к каталогу, где будут сохраняться обученная модель и другие выходные данные.\n",
    "        per_device_train_batch_size=4, # Размер пакета (batch size) для обучения на каждом устройстве. Определяет количество образцов данных, обрабатываемых за один шаг обучения на каждом устройстве.\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=1, # Количество шагов накопления градиента. Это позволяет эффективно увеличить размер пакета, не увеличивая использование памяти.\n",
    "        gradient_checkpointing=True, # чекпоинты градиентов для сохранения памяти\n",
    "        weight_decay=0.001,\n",
    "        optim=\"paged_adamw_32bit\", # 8-16-32\n",
    "        learning_rate=1e-4,\n",
    "        lr_scheduler_type=\"cosine\", # \"constant\"\n",
    "        # fp16=False,\n",
    "        # bf16=False,\n",
    "        # tf32=True,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        save_strategy=\"steps\", # Стратегия сохранения модели. \"epoch\" означает, что модель будет сохраняться после каждой эпохи обучения.\n",
    "        logging_steps=100, # Количество шагов обучения между логированием метрик обучения.\n",
    "        num_train_epochs=1, # 1-2-3\n",
    "        # eval_steps=50,\n",
    "        save_steps=500,\n",
    "        max_steps=-1, # Максимальное количество шагов обучения. Обучение закончится, когда будет достигнуто это число шагов, даже если не все эпохи были завершены.\n",
    "        # report_to=\"tensorboard\",\n",
    "        # push_to_hub=True\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        args=training_arguments,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # dataset_text_field=\"messages\", # не передаем это поле, если датасет уже был предобработан в нужный формат\n",
    "        # dataset_kwargs={\n",
    "        #     \"add_special_tokens\": False,  # We template with special tokens\n",
    "        #     \"append_concat_token\": False, # No need to add additional separator token\n",
    "        # },\n",
    "        neftune_noise_alpha=5,\n",
    "        # packing=True,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=512,\n",
    "        # neftune_noise_alpha=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model(\"./lora_adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединение модели с LoRA-адаптером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.80s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "lora_adapters = \"./lora_adapters\"\n",
    "merged_model_path = \"./merged-model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, lora_adapters)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "model = model.merge_and_unload()\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model.save_pretrained(merged_model_path, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "# tokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка на HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 28.0/28.0 [00:00<00:00, 110kB/s]\n",
      "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 16.4k/4.94G [00:00<23:26:25, 58.6kB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 2.34M/4.94G [00:00<11:28, 7.17MB/s]   \n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 5.88M/4.94G [00:00<05:11, 15.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 13.3M/4.94G [00:00<03:36, 22.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 16.0M/4.94G [00:01<06:13, 13.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 30.8M/4.94G [00:01<03:46, 21.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 33.4M/4.94G [00:02<06:00, 13.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 43.2M/4.94G [00:02<04:35, 17.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 46.7M/4.94G [00:02<04:14, 19.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 59.8M/4.94G [00:03<03:19, 24.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|▏         | 72.0M/4.94G [00:03<02:52, 28.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 79.3M/4.94G [00:04<03:54, 20.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 94.8M/4.94G [00:05<03:46, 21.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 97.5M/4.94G [00:05<06:09, 13.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 107M/4.94G [00:05<03:27, 23.3MB/s] \n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 119M/4.94G [00:06<03:01, 26.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 123M/4.94G [00:06<03:30, 22.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 138M/4.94G [00:07<03:10, 25.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 160M/4.94G [00:07<02:59, 26.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 172M/4.94G [00:08<02:21, 33.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▎         | 176M/4.94G [00:08<04:10, 19.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 186M/4.94G [00:08<02:47, 28.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 204M/4.94G [00:09<02:22, 33.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▍         | 224M/4.94G [00:10<02:50, 27.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▍         | 229M/4.94G [00:10<03:41, 21.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▍         | 240M/4.94G [00:11<03:23, 23.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▌         | 252M/4.94G [00:11<02:31, 31.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▌         | 265M/4.94G [00:12<02:56, 26.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   6%|▌         | 288M/4.94G [00:12<02:31, 30.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   6%|▌         | 298M/4.94G [00:13<03:10, 24.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   6%|▋         | 314M/4.94G [00:13<02:20, 33.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 330M/4.94G [00:14<01:59, 38.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 346M/4.94G [00:14<01:51, 41.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 364M/4.94G [00:15<01:36, 47.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 380M/4.94G [00:15<01:35, 47.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 394M/4.94G [00:15<01:40, 45.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 416M/4.94G [00:16<02:13, 33.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▊         | 432M/4.94G [00:17<02:11, 34.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▉         | 443M/4.94G [00:17<01:40, 44.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▉         | 458M/4.94G [00:17<01:48, 41.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  10%|▉         | 480M/4.94G [00:18<01:37, 45.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  10%|▉         | 486M/4.94G [00:18<02:17, 32.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  10%|▉         | 492M/4.94G [00:18<02:15, 32.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 656M/4.94G [00:23<02:12, 32.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 665M/4.94G [00:24<01:43, 41.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  14%|█▎        | 672M/4.94G [00:24<02:06, 33.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  14%|█▍        | 701M/4.94G [00:24<01:42, 41.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  14%|█▍        | 712M/4.94G [00:25<02:05, 33.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▍        | 722M/4.94G [00:25<02:26, 28.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▍        | 733M/4.94G [00:26<01:50, 38.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▌        | 749M/4.94G [00:26<01:53, 36.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▌        | 760M/4.94G [00:26<02:03, 33.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▌        | 766M/4.94G [00:26<01:49, 38.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  16%|█▌        | 776M/4.94G [00:27<02:00, 34.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  16%|█▌        | 797M/4.94G [00:28<02:03, 33.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  16%|█▋        | 808M/4.94G [00:28<02:12, 31.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 819M/4.94G [00:28<02:33, 26.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 830M/4.94G [00:29<01:52, 36.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 846M/4.94G [00:29<01:46, 38.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 856M/4.94G [00:29<02:05, 32.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 872M/4.94G [00:30<01:59, 34.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 878M/4.94G [00:30<01:43, 39.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 895M/4.94G [00:31<01:58, 34.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 910M/4.94G [00:31<01:48, 37.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  19%|█▊        | 926M/4.94G [00:32<01:42, 39.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  19%|█▉        | 935M/4.94G [00:32<02:14, 29.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  19%|█▉        | 941M/4.94G [00:32<01:51, 36.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  19%|█▉        | 960M/4.94G [00:33<01:53, 35.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|█▉        | 972M/4.94G [00:33<02:05, 31.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|█▉        | 983M/4.94G [00:34<02:06, 31.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|█▉        | 988M/4.94G [00:34<01:55, 34.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|██        | 999M/4.94G [00:34<02:33, 25.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|██        | 1.00G/4.94G [00:34<02:15, 29.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  20%|██        | 1.01G/4.94G [00:35<02:43, 24.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.02G/4.94G [00:35<02:01, 32.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.04G/4.94G [00:36<01:59, 32.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.04G/4.94G [00:36<03:00, 21.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  21%|██▏       | 1.05G/4.94G [00:36<02:03, 31.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.07G/4.94G [00:37<01:49, 35.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.08G/4.94G [00:37<01:59, 32.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.08G/4.94G [00:37<01:48, 35.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.10G/4.94G [00:38<01:49, 35.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.12G/4.94G [00:38<01:49, 35.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.13G/4.94G [00:39<02:10, 29.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.14G/4.94G [00:39<02:45, 23.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.15G/4.94G [00:40<02:52, 22.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▎       | 1.16G/4.94G [00:40<01:48, 34.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.18G/4.94G [00:41<01:59, 31.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.19G/4.94G [00:42<02:43, 22.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.20G/4.94G [00:42<02:45, 22.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  25%|██▍       | 1.21G/4.94G [00:42<02:25, 25.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  25%|██▍       | 1.23G/4.94G [00:43<01:55, 32.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  25%|██▍       | 1.23G/4.94G [00:43<02:37, 23.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  25%|██▌       | 1.24G/4.94G [00:43<01:48, 34.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.26G/4.94G [00:44<01:51, 33.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.27G/4.94G [00:44<02:00, 30.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.28G/4.94G [00:45<02:23, 25.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.29G/4.94G [00:45<01:42, 35.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▋       | 1.31G/4.94G [00:46<01:40, 36.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.32G/4.94G [00:46<01:53, 31.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.33G/4.94G [00:46<02:10, 27.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.34G/4.94G [00:47<01:37, 37.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.36G/4.94G [00:47<01:40, 35.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.37G/4.94G [00:47<01:49, 32.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.38G/4.94G [00:48<02:23, 24.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.39G/4.94G [00:48<01:47, 33.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.40G/4.94G [00:49<01:38, 35.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  29%|██▊       | 1.42G/4.94G [00:49<01:37, 36.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  29%|██▉       | 1.43G/4.94G [00:50<01:44, 33.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  29%|██▉       | 1.44G/4.94G [00:50<02:05, 27.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  29%|██▉       | 1.45G/4.94G [00:50<01:33, 37.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|██▉       | 1.47G/4.94G [00:51<01:43, 33.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|██▉       | 1.48G/4.94G [00:51<02:01, 28.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|███       | 1.49G/4.94G [00:52<02:09, 26.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|███       | 1.50G/4.94G [00:52<01:29, 38.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.51G/4.94G [00:53<02:13, 25.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.52G/4.94G [00:53<01:52, 30.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.52G/4.94G [00:53<02:49, 20.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.53G/4.94G [00:53<02:08, 26.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███▏      | 1.55G/4.94G [00:54<01:37, 35.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.56G/4.94G [00:54<02:04, 27.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.56G/4.94G [00:55<02:39, 21.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.57G/4.94G [00:55<04:04, 13.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.58G/4.94G [00:55<02:16, 24.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.59G/4.94G [00:56<02:27, 22.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.60G/4.94G [00:56<02:23, 23.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.61G/4.94G [00:56<01:32, 36.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.63G/4.94G [00:57<01:45, 31.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.63G/4.94G [00:57<02:36, 21.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.64G/4.94G [00:58<01:47, 30.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▎      | 1.66G/4.94G [00:58<01:33, 35.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▍      | 1.67G/4.94G [00:59<01:45, 30.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▍      | 1.68G/4.94G [00:59<02:32, 21.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▍      | 1.70G/4.94G [00:59<01:37, 33.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  35%|███▍      | 1.71G/4.94G [01:00<01:49, 29.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  35%|███▍      | 1.71G/4.94G [01:00<02:28, 21.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  35%|███▍      | 1.72G/4.94G [01:00<01:35, 33.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  35%|███▌      | 1.74G/4.94G [01:01<01:33, 34.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  35%|███▌      | 1.74G/4.94G [01:01<02:11, 24.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.76G/4.94G [01:02<01:31, 34.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.77G/4.94G [01:02<01:44, 30.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.78G/4.94G [01:03<02:19, 22.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.79G/4.94G [01:03<01:39, 31.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.80G/4.94G [01:03<01:29, 35.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.82G/4.94G [01:04<01:40, 31.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.82G/4.94G [01:04<01:48, 28.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.84G/4.94G [01:04<01:40, 31.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.85G/4.94G [01:05<01:51, 27.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.86G/4.94G [01:05<02:11, 23.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.87G/4.94G [01:06<01:25, 35.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.88G/4.94G [01:06<01:49, 27.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.89G/4.94G [01:07<01:50, 27.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.90G/4.94G [01:07<01:40, 30.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.92G/4.94G [01:07<01:37, 30.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.92G/4.94G [01:08<02:18, 21.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.94G/4.94G [01:08<01:34, 31.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.94G/4.94G [01:08<01:50, 27.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  40%|███▉      | 1.96G/4.94G [01:09<01:44, 28.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  40%|███▉      | 1.97G/4.94G [01:09<01:26, 34.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  40%|████      | 1.98G/4.94G [01:10<01:47, 27.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  40%|████      | 1.99G/4.94G [01:10<02:16, 21.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  40%|████      | 2.00G/4.94G [01:10<01:29, 32.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  41%|████      | 2.01G/4.94G [01:11<02:00, 24.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  41%|████      | 2.02G/4.94G [01:11<02:03, 23.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  41%|████      | 2.03G/4.94G [01:12<01:32, 31.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  41%|████▏     | 2.04G/4.94G [01:12<01:54, 25.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  41%|████▏     | 2.05G/4.94G [01:13<02:21, 20.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.06G/4.94G [01:13<01:33, 30.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.08G/4.94G [01:13<01:25, 33.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.09G/4.94G [01:14<01:27, 32.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.10G/4.94G [01:14<02:03, 23.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.11G/4.94G [01:14<01:26, 33.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.12G/4.94G [01:15<01:24, 33.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.14G/4.94G [01:16<01:23, 33.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▎     | 2.15G/4.94G [01:16<01:29, 31.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▎     | 2.16G/4.94G [01:16<01:54, 24.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.17G/4.94G [01:17<01:21, 34.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.18G/4.94G [01:17<01:43, 26.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.20G/4.94G [01:18<01:42, 26.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  45%|████▍     | 2.21G/4.94G [01:18<01:59, 22.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  45%|████▍     | 2.22G/4.94G [01:18<01:19, 34.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  45%|████▌     | 2.24G/4.94G [01:19<01:30, 30.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  45%|████▌     | 2.24G/4.94G [01:19<02:03, 21.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.25G/4.94G [01:20<01:23, 32.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.27G/4.94G [01:20<01:28, 30.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.28G/4.94G [01:21<01:30, 29.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▋     | 2.29G/4.94G [01:21<01:58, 22.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.30G/4.94G [01:21<01:17, 34.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.32G/4.94G [01:22<01:15, 34.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.33G/4.94G [01:22<01:14, 35.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.34G/4.94G [01:23<01:49, 23.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.35G/4.94G [01:23<01:45, 24.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.36G/4.94G [01:23<01:14, 34.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.38G/4.94G [01:24<01:14, 34.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.39G/4.94G [01:24<01:21, 31.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.40G/4.94G [01:24<01:14, 34.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▊     | 2.41G/4.94G [01:25<01:31, 27.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.41G/4.94G [01:25<02:05, 20.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.43G/4.94G [01:26<01:45, 24.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.44G/4.94G [01:27<01:53, 22.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.44G/4.94G [01:27<01:31, 27.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  50%|████▉     | 2.46G/4.94G [01:27<01:20, 30.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  50%|████▉     | 2.46G/4.94G [01:28<01:50, 22.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  50%|█████     | 2.49G/4.94G [01:28<01:08, 36.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  51%|█████     | 2.50G/4.94G [01:29<01:13, 33.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  51%|█████     | 2.51G/4.94G [01:29<01:06, 36.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  51%|█████     | 2.52G/4.94G [01:29<01:04, 37.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.61G/4.94G [01:32<01:19, 29.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.62G/4.94G [01:32<01:08, 33.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.63G/4.94G [01:32<01:20, 28.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.64G/4.94G [01:33<01:19, 28.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.66G/4.94G [01:33<01:12, 31.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.67G/4.94G [01:34<00:55, 40.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.69G/4.94G [01:34<01:06, 34.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▍    | 2.70G/4.94G [01:35<01:04, 35.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▍    | 2.71G/4.94G [01:35<01:17, 28.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▍    | 2.72G/4.94G [01:35<01:02, 35.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▌    | 2.73G/4.94G [01:36<01:09, 31.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▌    | 2.74G/4.94G [01:36<01:52, 19.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.74G/4.94G [01:36<01:30, 24.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.75G/4.94G [01:37<01:44, 20.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.76G/4.94G [01:37<01:23, 26.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.77G/4.94G [01:38<01:45, 20.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▋    | 2.78G/4.94G [01:38<01:04, 33.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▋    | 2.79G/4.94G [01:40<04:33, 7.89MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.80G/4.94G [01:40<02:43, 13.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.81G/4.94G [01:41<02:43, 13.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.81G/4.94G [01:41<01:59, 17.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.83G/4.94G [01:42<01:13, 28.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.84G/4.94G [01:42<01:17, 27.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.85G/4.94G [01:42<01:28, 23.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.86G/4.94G [01:43<01:07, 30.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.87G/4.94G [01:43<01:23, 24.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.88G/4.94G [01:44<01:08, 30.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.89G/4.94G [01:44<01:22, 25.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▊    | 2.90G/4.94G [01:45<01:43, 19.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.91G/4.94G [01:45<01:19, 25.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.92G/4.94G [01:46<01:40, 20.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.93G/4.94G [01:46<01:18, 25.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.93G/4.94G [01:46<01:29, 22.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.94G/4.94G [01:47<01:12, 27.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.96G/4.94G [01:47<01:17, 25.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.96G/4.94G [01:48<01:47, 18.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|██████    | 2.97G/4.94G [01:48<01:13, 26.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|██████    | 2.98G/4.94G [01:48<01:16, 25.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 2.99G/4.94G [01:49<01:29, 21.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.00G/4.94G [01:49<01:06, 29.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.01G/4.94G [01:49<01:08, 28.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.02G/4.94G [01:50<01:04, 29.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████▏   | 3.04G/4.94G [01:50<01:05, 29.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.05G/4.94G [01:51<01:11, 26.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.05G/4.94G [01:51<01:05, 28.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.07G/4.94G [01:51<01:03, 29.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.07G/4.94G [01:52<01:25, 21.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.09G/4.94G [01:52<01:01, 30.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.09G/4.94G [01:53<01:15, 24.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.10G/4.94G [01:53<01:30, 20.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.12G/4.94G [01:53<01:03, 29.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.12G/4.94G [01:54<01:47, 16.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.14G/4.94G [01:55<01:29, 20.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  64%|██████▎   | 3.15G/4.94G [01:55<01:16, 23.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.16G/4.94G [01:56<01:11, 25.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.17G/4.94G [01:56<00:59, 29.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.18G/4.94G [01:56<01:03, 27.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▍   | 3.19G/4.94G [01:57<01:06, 26.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▍   | 3.20G/4.94G [01:57<01:00, 28.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.22G/4.94G [01:58<00:57, 30.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.23G/4.94G [01:58<01:03, 27.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.23G/4.94G [01:59<01:27, 19.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.24G/4.94G [01:59<01:00, 28.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.26G/4.94G [01:59<00:57, 29.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.27G/4.94G [02:00<01:10, 23.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.27G/4.94G [02:00<01:05, 25.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▋   | 3.28G/4.94G [02:00<01:02, 26.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.29G/4.94G [02:01<00:56, 29.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.30G/4.94G [02:01<01:24, 19.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.31G/4.94G [02:01<00:52, 31.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.32G/4.94G [02:03<02:35, 10.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.33G/4.94G [02:03<01:19, 20.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.35G/4.94G [02:04<01:03, 25.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.36G/4.94G [02:04<01:11, 22.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.38G/4.94G [02:05<00:48, 32.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.38G/4.94G [02:05<00:59, 26.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  69%|██████▊   | 3.39G/4.94G [02:05<00:55, 28.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.40G/4.94G [02:06<01:04, 24.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.41G/4.94G [02:07<01:08, 22.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.43G/4.94G [02:07<01:18, 19.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|██████▉   | 3.44G/4.94G [02:08<00:54, 27.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|██████▉   | 3.45G/4.94G [02:08<00:54, 27.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|██████▉   | 3.45G/4.94G [02:08<00:52, 28.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|███████   | 3.47G/4.94G [02:09<01:01, 24.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|███████   | 3.47G/4.94G [02:09<01:16, 19.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  71%|███████   | 3.49G/4.94G [02:09<00:47, 30.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  71%|███████   | 3.50G/4.94G [02:10<00:49, 29.1MB/s]\n",
      "\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 4.54G/4.54G [02:10<00:00, 34.7MB/s]\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 5.00G/5.00G [02:34<00:00, 32.4MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.94G/4.94G [02:59<00:00, 27.5MB/s]\n",
      "\n",
      "Upload 3 LFS files: 100%|██████████| 3/3 [03:00<00:00, 60.08s/it] \n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 745kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aesedeu/openorca-7b-stlt-chatbot/commit/29b1ef30c812b79fb48d66f9eb17462091c9a702', commit_message='Upload tokenizer', commit_description='', oid='29b1ef30c812b79fb48d66f9eb17462091c9a702', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# !huggingface-cli login\n",
    "hf_repo_name = \"aesedeu/openorca-7b-stlt-chatbot\"\n",
    "\n",
    "model.push_to_hub(\n",
    "    hf_repo_name,\n",
    "    check_pr=True,\n",
    "    max_shard_size=\"5GB\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    hf_repo_name,\n",
    "    check_pr=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### модель + адаптеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "lora_adapters = \"./lora_adapters\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "  model,\n",
    "  lora_adapters,\n",
    "  torch_dtype=torch.float16,\n",
    "  device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка через LoRA-адаптеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.28s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "lora_adapters = \"./lora_adapters\"\n",
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  lora_adapters,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка объединенной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "merged_model_path = \"./merged-model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  merged_model_path,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инференс вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"../Mistral-7B-OpenOrca\" \n",
    "\n",
    "# Настройка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>system\\nТы - русскоязычный ассистент Степан. Отвечаешь только на вопросы о лотереях.</s>\\n<s>user\\nКак вернуть билет?</s>\\n<s>assistant\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"Ты - русскоязычный ассистент Степан. Отвечаешь только на вопросы о лотереях.\"\"\"\n",
    "# SYSTEM_PROMPT = f\"\"\"Ты - пьяный пират, который ищет свой корабль. Разговаривай как пьяный пират.\"\"\"\n",
    "\n",
    "QUESTION = 'Как вернуть билет?'\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{QUESTION}\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"\"}\n",
    "]\n",
    "\n",
    "input_message = \"\"\n",
    "for i in chat:\n",
    "    input_message += tokenizer.bos_token + i['role'] + '\\n' + i['content'] + tokenizer.eos_token + '\\n'\n",
    "input_message = input_message[:-5].strip() + \"\\n\"\n",
    "input_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В случаи возврата бонсов без выплаты по бонусной игры, список участников и результат розагружается отдельно в раздела «Все участия» не ранее чем через двадцати четверги после завершения турнира.  Для проведения технического обслуживания клиентов можем быть требуться короткие передышки из делательности примерно минут. Мы обращаем ваше внимание на то, что способ получениябо\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "input_data = tokenizer(input_message, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_data = {k: v.to(\"cuda:0\") for k, v in input_data.items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    bos_token_id = 1,\n",
    "    do_sample = True,\n",
    "    eos_token_id = 2,\n",
    "    max_length = 2048,\n",
    "    max_new_tokens = 128,\n",
    "    repetition_penalty=1.3,\n",
    "    # length_penalty=0.1,\n",
    "    no_repeat_ngram_size=15,\n",
    "    pad_token_id = 2,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    top_k= 40,\n",
    "    # low_memory=True\n",
    ")\n",
    "\n",
    "output_data = model.generate(\n",
    "    **input_data,\n",
    "    generation_config = generation_config\n",
    ")[0]\n",
    "\n",
    "output_data = output_data[len(input_data[\"input_ids\"][0]):]\n",
    "output_message = tokenizer.decode(output_data, skip_special_tokens=True)\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инференс с помощью pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "eval_dataset = load_dataset(\n",
    "    path=\"./dataset\",\n",
    "    # data_files=\"test_dataset.json\", # убираем если нужно загрузить train+test\n",
    "    split=\"test\" # убираем если нужно загрузить train+test\n",
    ")\n",
    "\n",
    "rand_idx = randint(0, len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Здравствуйте Как проверить билет В телевизоре повторно покажут сегодняшний тираж Хорошо Ну что\n",
      "==================================\n",
      "Original Answer:\n",
      "  Для проверки билета, пожалуйста, перейдите по ссылке https://www.stoloto/check-ticket\n",
      "==================================\n",
      "Generated Answer:\n",
      "Для проверки билета, пожалуйста, перейдите по ссылке https://www.stoloto/check-ticket  Если при покупке билета Вы не указывали номер телефона, то номер тиража и билета указаны на самом билете. Ознакомиться с инструкцией, как принять участие в лотерее на сайте, Вам не нужно (https://www.stoloto/lottery/onlinesale?from=header)   Для проверки билета, пожалуйста, перейдите по ссылке https://www.stoloto/check-ticket  Если при покупке билета Вы не указывали номер телефона, то номер тиража и билета указаны на самом билете. Ознакомиться с инструкцией, как принять участие в лотерее на сайте, Вам не нужно (https://www.stoloto\n"
     ]
    }
   ],
   "source": [
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    # top_k=40,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "    pad_token_id=pipe.tokenizer.pad_token_id,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    # bos_token_id=1,\n",
    "    # eos_token_id=2\n",
    ")\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(\"==================================\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(\"==================================\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
