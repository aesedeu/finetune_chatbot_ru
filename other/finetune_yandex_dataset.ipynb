{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.philschmid.de/fine-tune-llms-in-2024-with-trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "pip install sentencepiece transformers trl datasets pandas numpy protobuf accelerate bitsandbytes flash-attn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "# должна быть '2.0.1+cu117'\n",
    "# pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA A100-PCIE-40GB', major=8, minor=0, total_memory=40384MB, multi_processor_count=108)\n",
      "tensor([-0.3421], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(0))\n",
    "print(torch.randn(1).cuda())\n",
    "# _CudaDeviceProperties(name='NVIDIA A100-PCIE-40GB', major=8, minor=0, total_memory=40384MB, multi_processor_count=108)\n",
    "# tensor([-0.2955], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, load_dataset_builder\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, AutoConfig, pipeline, GenerationConfig, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "login(\n",
    "  token=os.getenv(\"HF_TOKEN\"),\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования промпта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You're AI assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm doing great. How can I help you today?<|im_end|>\n",
      "<|im_start|>user\n",
      "I'd like to show off how chat templating works!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id=\"Open-Orca/Mistral-7B-OpenOrca\" # pip install sentencepiece\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You're AI assistant\"},\n",
    "   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset_5000.csv')\n",
    "\n",
    "dataset = {'question': dataset['question'].to_list(), 'answer': dataset['answer'].to_list()}\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8472.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Действительно ли все в этом мире имеет свою причину?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Никак не иначе. Причины чего именно Вы ставите под сомнение? Составьте список - и я Вам все причины назову. А если не \"влом\" - обясните: как представляете себе что-то без причины?',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['messages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.', 'role': 'system'}, {'content': 'Где лучше всего по вашему мнению хранить свои пароли?', 'role': 'user'}, {'content': 'Самый лучший способ - в голове.\\nЧуть похуже - на бумажке, которая убрана в недоступное для посторонних место.\\nЕсть различные менеджеры хранения паролей (например, KeePass или LastPass).\\nМожно просто хранить в файле (в любом текстовом редакторе).\\n.\\nСамый надежный - это хранить в голове. Менее надёжный - использовать менеджеры.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=1/5)\n",
    "print(dataset[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 13.77ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 15.33ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4598874"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохранение датасета\n",
    "dataset['train'].to_json(\"./dataset/train_dataset.json\", orient=\"records\")\n",
    "dataset['test'].to_json(\"./dataset/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4000 examples [00:00, 71022.79 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"./dataset\",\n",
    "    data_files=\"train_dataset.json\", # убираем если нужно загрузить train+test\n",
    "    split=\"train\" # убираем если нужно загрузить train+test\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чем не занимается инженер-строитель?</td>\n",
       "      <td>В каждой компании по разному! Но с уверенностью могу заявить, что инженер-строитель не управляет строительными машинами (а именно не сидит за рулем) и сам не производит строительные работы (не пачкает руки)\\nКакой вопрос, такой ответ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Какие витамины содержат кремний?</td>\n",
       "      <td>Кремний содержится в популярных витаминно -минеральных комплексах с кремнием, например, Доппельгерц актив, Витрум. Также кремний содержится в специально разработанных витаминах для восполнения дефицита кремния, например, Leader Vahva Piimaa, Piimax C + Biotini, Balance drink Si+, Piimax Pro-Vita.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Почему в России певцы, актеры, шоумены все время «сбиваются» в стайки на концерты, огоньки, корпоративы. А в Европе и США — каждая звезда сама по себе?</td>\n",
       "      <td>Боюсь ошибиться, не сильна в теме телевидения и шоу-бизнеса, но предположу.\\nМне кажется, многие форматы публичных  - телевизионных и не только - выступлений артистов, в частности, музыкантов, пришли по большей части с запада, потому все, что так или иначе есть \"у нас\", изначально было или есть/остается также и \"у них\".\\n1) Концерты.\\n- всякие церемонии вручения музыкальных премий как вариант....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  question  \\\n",
       "0                                                                                                                     Чем не занимается инженер-строитель?   \n",
       "1                                                                                                                         Какие витамины содержат кремний?   \n",
       "2  Почему в России певцы, актеры, шоумены все время «сбиваются» в стайки на концерты, огоньки, корпоративы. А в Европе и США — каждая звезда сама по себе?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                            answer  \n",
       "0                                                                                                                                                                     В каждой компании по разному! Но с уверенностью могу заявить, что инженер-строитель не управляет строительными машинами (а именно не сидит за рулем) и сам не производит строительные работы (не пачкает руки)\\nКакой вопрос, такой ответ...  \n",
       "1                                                                                                        Кремний содержится в популярных витаминно -минеральных комплексах с кремнием, например, Доппельгерц актив, Витрум. Также кремний содержится в специально разработанных витаминах для восполнения дефицита кремния, например, Leader Vahva Piimaa, Piimax C + Biotini, Balance drink Si+, Piimax Pro-Vita.  \n",
       "2  Боюсь ошибиться, не сильна в теме телевидения и шоу-бизнеса, но предположу.\\nМне кажется, многие форматы публичных  - телевизионных и не только - выступлений артистов, в частности, музыкантов, пришли по большей части с запада, потому все, что так или иначе есть \"у нас\", изначально было или есть/остается также и \"у них\".\\n1) Концерты.\\n- всякие церемонии вручения музыкальных премий как вариант....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset_5000.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Чем не занимается инженер-строитель?'}, {'role': 'assistant', 'content': 'В каждой компании по разному! Но с уверенностью могу заявить, что инженер-строитель не управляет строительными машинами (а и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Какие витамины содержат кремний?'}, {'role': 'assistant', 'content': 'Кремний содержится в популярных витаминно -минеральных комплексах с кремнием, например, Доппельгерц актив, Витрум. Также кремний...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Почему в России певцы, актеры, шоумены все время «сбиваются» в стайки на концерты, огоньки, корпоративы. А в Европе и США — каждая звезда сама по себе?'}, {'role': 'assistant', 'content': 'Боюсь оши...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                          messages\n",
       "0  [{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Чем не занимается инженер-строитель?'}, {'role': 'assistant', 'content': 'В каждой компании по разному! Но с уверенностью могу заявить, что инженер-строитель не управляет строительными машинами (а и...\n",
       "1  [{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Какие витамины содержат кремний?'}, {'role': 'assistant', 'content': 'Кремний содержится в популярных витаминно -минеральных комплексах с кремнием, например, Доппельгерц актив, Витрум. Также кремний...\n",
       "2  [{'role': 'system', 'content': 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'}, {'role': 'user', 'content': 'Почему в России певцы, актеры, шоумены все время «сбиваются» в стайки на концерты, огоньки, корпоративы. А в Европе и США — каждая звезда сама по себе?'}, {'role': 'assistant', 'content': 'Боюсь оши..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = 'Ты ассистент-помощник, который отвечает на вопросы человека. Ты должен быть вежлив и точен в своих ответах. Отвечай кратко и понятно.'\n",
    "\n",
    "df['messages'] = df[['question', 'answer']].apply(lambda x: np.array([{'role':'system', 'content': SYSTEM_PROMPT},\n",
    "                                                                      {'role':'user', 'content': x['question']},\n",
    "                                                                      {'role':'assistant', 'content': x['answer']}]), axis=1)\n",
    "\n",
    "\n",
    "# НЕ НУЖНО ПРИМЕНЯТЬ ЧАТ ТЕМПЛЕЙТ!!!\n",
    "# df['messages'] = df['chat_format'].apply(lambda x: tokenizer.apply_chat_template(x, tokenize=False))\n",
    "# НЕ НУЖНО ПРИМЕНЯТЬ ЧАТ ТЕМПЛЕЙТ!!!\n",
    "\n",
    "\n",
    "train_dataset = df[['messages']]\n",
    "train_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset.from_pandas(train_dataset)\n",
    "data = data.shuffle()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.train_test_split(test_size=1/5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 32.82ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 42.73ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4339678"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохранение датасета\n",
    "data['train'].to_json(\"./dataset/train_dataset.json\", orient=\"records\")\n",
    "data['test'].to_json(\"./dataset/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4000 examples [00:00, 154025.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"./dataset\",\n",
    "    data_files=\"train_dataset.json\", # убираем если нужно загрузить train+test\n",
    "    split=\"train\" # убираем если нужно загрузить train+test\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"./Mistral-7B-OpenOrca\" # модель, которую будем дообучать\n",
    "lora_model=\"./OpenOrca-7b-aesedeu-lora\" # директория с LoRA-адаптерами, которые получим на выходе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'right' # to prevent warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Устанавливает токен для дополнения (pad_token) равным токену конца строки (eos_token). Это полезно для моделей, которые используют один и тот же токен для обозначения конца строки и дополнения.\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=False, # Загружает модель в 4-битном формате для уменьшения использования памяти.\n",
    "        load_in_8bit=True,\n",
    "        bnb_4bit_quant_type=\"fp4\", # Указывает тип квантования, в данном случае \"nf4\" (nf4/dfq/qat/ptq/fp4)\n",
    "        bnb_4bit_compute_dtype=\"float16\", # Устанавливает тип данных для вычислений в 4-битном формате как float16.\n",
    "        bnb_4bit_use_double_quant=False # Указывает, что не используется двойное квантование.\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    model.config.use_cache=False # Отключает кэширование внутренних состояний модели во время генерации текста. Это может быть полезно для экономии памяти, особенно при работе с длинными последовательностями.\n",
    "    model.config.pretraining_tp=1 # параметр, связанный с техниками распределенного обучения \n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:01<00:00, 2937.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# НАСТРОИТЬ!!!\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=8, # определяет ранг матриц адаптации в LoRA. Этот параметр контролирует количество параметров адаптации, которые добавляются к модели. Более низкий ранг означает меньше дополнительных параметров и, следовательно, меньшее влияние на исходные веса модели.\n",
    "        lora_alpha=32, # масштабирующий коэффициент для матриц адаптации. Этот коэффициент управляет степенью, с которой адаптированные веса влияют на поведение модели.\n",
    "        # lora_dropout=0.05,\n",
    "        bias=\"none\", # указывает, должен ли добавляться смещение (bias) к параметрам LoRA\n",
    "        task_type=\"CAUSAL_LM\", # \"CAUSAL_LM\" означает причинно-следственное (каузальное) языковое моделирование ????????????\n",
    "        # target_modules=\"all-linear\"\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=lora_model, # Путь к каталогу, где будут сохраняться обученная модель и другие выходные данные.\n",
    "        per_device_train_batch_size=2, # Размер пакета (batch size) для обучения на каждом устройстве. Определяет количество образцов данных, обрабатываемых за один шаг обучения на каждом устройстве.\n",
    "        gradient_accumulation_steps=3, # Количество шагов накопления градиента. Это позволяет эффективно увеличить размер пакета, не увеличивая использование памяти.\n",
    "        gradient_checkpointing=True, # чекпоинты градиентов для сохранения памяти\n",
    "        optim=\"adamw_torch\", # Оптимизатор, используемый для обучения. В данном случае используется \"paged_adamw_32bit\", что представляет собой определённую версию оптимизатора AdamW с 32-битной точностью.\n",
    "        learning_rate=2e-4, # Скорость обучения. Определяет, насколько сильно веса модели обновляются во время обучения.\n",
    "        lr_scheduler_type=\"cosine\", # \"constant\" / Тип планировщика скорости обучения. \"cosine\" означает использование косинусного расписания с понижением скорости обучения.\n",
    "        bf16=True, # use bfloat16 precision\n",
    "        tf32=True, # use tf32 precision\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        save_strategy=\"steps\", # Стратегия сохранения модели. \"epoch\" означает, что модель будет сохраняться после каждой эпохи обучения.\n",
    "        logging_steps=10, # Количество шагов обучения между логированием метрик обучения.\n",
    "        num_train_epochs=3, # Количество эпох обучения, то есть сколько раз обучающий набор данных будет проходить через модель.\n",
    "        # eval_steps=50,\n",
    "        save_steps=100,\n",
    "        max_steps=500, # Максимальное количество шагов обучения. Обучение закончится, когда будет достигнуто это число шагов, даже если не все эпохи были завершены.\n",
    "        # report_to=\"tensorboard\",\n",
    "        # push_to_hub=True\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        args=training_arguments,\n",
    "        train_dataset=dataset,\n",
    "        # eval_dataset=dataset,\n",
    "        # dataset_text_field=\"messages\", # не передаем это поле, если датасет уже был предобработан в нужный формат\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False, # No need to add additional separator token\n",
    "        },\n",
    "        # packing=True,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=2048\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 37:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.355600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.324800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in ./Mistral-7B-OpenOrca - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# free the memory again\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объединение модели с LoRA-адаптером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"./Mistral-7B-OpenOrca\" # модель, которую будем дообучать\n",
    "lora_model=\"./OpenOrca-7b-aesedeu-lora\" # директория с LoRA-адаптерами, которые получим на выходе\n",
    "lora_adapters = lora_model + \"/checkpoint-500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./OpenOrca-7b-aesedeu-lora-merged/tokenizer_config.json',\n",
       " './OpenOrca-7b-aesedeu-lora-merged/special_tokens_map.json',\n",
       " './OpenOrca-7b-aesedeu-lora-merged/tokenizer.model',\n",
       " './OpenOrca-7b-aesedeu-lora-merged/added_tokens.json',\n",
       " './OpenOrca-7b-aesedeu-lora-merged/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_path = lora_model + \"-merged\"\n",
    "\n",
    "### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "config = PeftConfig.from_pretrained(lora_adapters)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_adapters)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, lora_adapters)\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    lora_adapters,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_model_path, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель + адаптеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id = \"./Mistral-7B-OpenOrca\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "  model,\n",
    "  \"./OpenOrca-7b-aesedeu-lora/checkpoint-500/\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## только адаптеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.28s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# peft_model_id = \"./OpenOrca-7b-aesedeu-lora-merged\"\n",
    "peft_model_id = \"OpenOrca-7b-aesedeu-lora/checkpoint-500\"\n",
    "model_id=\"./Mistral-7B-OpenOrca\" \n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## объединенная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id = \"./OpenOrca-7b-aesedeu-lora-merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"cuda\",\n",
    "  # load_in_8bit=True,\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "eval_dataset = load_dataset(\n",
    "    path=\"./dataset\",\n",
    "    data_files=\"test_dataset.json\", # убираем если нужно загрузить train+test\n",
    "    split=\"train\" # убираем если нужно загрузить train+test\n",
    ")\n",
    "\n",
    "rand_idx = randint(0, len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/ml_projects/chatbot/3_model_mistralorca/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "    pad_token_id=pipe.tokenizer.pad_token_id,\n",
    "    # no_repeat_ngram_size=15,\n",
    "    # bos_token_id=1,\n",
    "    # eos_token_id=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Торжество православия значит - торговля в храмах РПЦ?\n",
      "==================================\n",
      "Original Answer:\n",
      "По вопросу сразу видно хейтера и хайпожора))) Хайповать, конечно не мешки ворочать. Если отвечать по существу, то Торжество Православия - праздник, который был установлен после решений последнего Вселенского Собора об иконопочитании в качестве аксиомы правосланого вероучения, а иконоборчество было осуждено как ересь (неправославная доктрина). Но ведь автора не это интересует, правда?\n",
      "==================================\n",
      "Generated Answer:\n",
      "Не совсем так... Всё дело не только историческими фактами (например: продажа мощевиков святынь), но также с текущией ситуационой рынке реализации духовных услуг как таковых! Поэтому если вы хотите осудиться церкви за это я ваш человек) Но все же стоит обращатся к истории для того чтобы разобрался что таки \"торги\" были при них или без их?! Ибо они далекий разумеется больше чем обязанности перед Богом!!! А себе доверь!! :) Есть много способы помочью своим согражданиным!!!! Спасительный путей избавок можемы найдено где либендю!) Осталось Вашим Выбора какой пастиль подбираем???) Надо будуче годы жизнеспособность людей росло она стала меньшей после распада СССР(Вот туманная стороны этого процесса)) Каким был он когда\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(\"==================================\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(\"==================================\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
